---
bibliography: references.bib
csl: ieee-with-url.csl
---

{{< include includes/_top_logo.qmd >}}

## About My Problem-Solving Course {.ch}

In one type of engineering problem-solving course[refs?], the instructor presents theories, derives and simplifies equations and shows how to use the theory and equations to answer questions about the performance or behavior of some process or system. The learning outcomes in this kind of problem-solving course center around the learner being able to formulate and solve equations to answer a variety of questions about systems or processes to which the theory applies. The problems in this kind of problem-solving course are typically “story problems.” That is, they take the form of a few written paragraphs that provide some information and then ask one or more questions.

In a common approach to teaching this type of problem-solving course, the instructor delivers didactic lectures in class. Homework problems are regularly assigned to the students. The homework problems are graded and returned to the students, and the homework grades become some fraction of the final course grade. At periodic intervals, the students take exams where they are asked to solve similar problems. The exams are graded and become a larger fraction of the final course grade. In this first of three papers, one particular example of this type of problem-solving course taught using ths common approach is assessed. A number of shortcomings are identified and discussed, and pedagogic changes to address those shortcomings are suggested. The second paper describes the the redesign of the overall course structure and its re-assessesment, and third paper describes the redesign of homework, feedback and exams and final assessment of the resulting course.

The course being assessed here is a required, upper-division chemical engineering course on chemical reaction kinetics and reaction engineering. Broadly, students learn to solve five types of problems in the course: rate expression generation (e.g. from a reaction mechanism), kinetics data analysis, isolated reactor modeling, reactor systems modeling and non-ideal reactor modeling. The same equations (mole and energy balances on ideal reactors) are used to solve the kinetics data analysis, isolated reactor modeling and reactor system modeling problem types, but the equations are used in different ways: parameter estimation, calculation of response, system optimization and system design. The course is offered once per year, in the fall semester. The author has taught all but 3 offerings (1998, 1999 and 2000) of this course since 1986. From 1986 through 2005 the didaactic lecture format described above was used, and the course offerings assessed here used that format.

### Course Offerings

Complete records including gradebooks, syllabi and instructor notes were available for ten offerings of the course between 1986 and 2005. De-identified gradebooks from those ten offerings were used for the assessment of the didactic lecture approach to teaching. For each student the homework submission rate (percent of assignments that were submitted), exams score (the average of all exam scores) and the deviation of the exams score from the class average exams score were computed. The instructor's notes from each offering were examined and relevant observations were extracted. Table \@ref(tab:offering-data) provides summary information about the course offerings used in the assessment. It may be noted that the number of homework assignments per course offering varied between 9 and 12 except for the 2005 offering where there were 22 homework assignments. This large increase was the result of switching from one assignment per week (with several problems per assignment) to one assignment per class meeting (with one problem per assignment). 

``{r offering-data}
load("../results/course_offering_summary.rda")
course_offering_summary$sub_rate <- signif(course_offering_summary$sub_rate,3)
course_offering_summary$exams_score <- signif(course_offering_summary$exams_score,3)
kbl(course_offering_summary, booktabs=TRUE, escape=FALSE, caption="Course Offerings Assessed",
    col.names=linebreak(c("Year","Number\nof\nStudents","Number\n of\nAssignments","Homework\nSubmission\nRate","Number\nof\nExams","Average\nExams\nScore"),align = "c"),
    align = "cccccc") %>% 
  kable_classic() %>% 
  kable_styling(latex_options = "hold_position",
                full_width = FALSE)
``

The learning objectives and information presented in the lectures was essentially the same every year. The table does not suggest a trend in exams score over time, but it does show year-to-year variation. This might be expected since no two offerings of the course are identical. Instead of comparing exams scores across multiple offerings of the course, the deviation of a student's exams score from the mean *for their course offering* can be used. The boxplot in Figure \@ref(fig:exams-scores) represents distributions of exams scores for the course offerings. In this figure the boxes enclose the interquartile range (25^th^ through 75^th^ percentiles), the horizontal lines within the boxes represent the median, the vertical lines span the first and fourth quartiles and the dots represent outliers. Figure \@ref(fig:deviations) shows the same information for the deviations of the students exams scores from the mean exams score for their course offering. The figures show that using deviations from the mean exams score eliminates much of the year-to-year variation.

``{r exams-scores, fig.cap="Distributions of Exams Scores by Course Offering"}
load("../data/by_offering_data.rda")
scoresBox <- ggplot(data = by_offering_data, aes(year,exam_score)) +
  geom_boxplot() +
  labs(x = "Year",
       y = "Exams Score (%)") 
scoresBox
``

&nbsp;

``{r deviations, fig.cap="Distributions of Exams Scores Deviations by Cours Offering"}
deviationsBox <- ggplot(data = by_offering_data, aes(year,dev)) +
  geom_boxplot() +
  labs(x = "Year",
       y = "Deviation from the Mean of the Course Offering Exams Scores (%)") 
deviationsBox
``

### The Purpose and Basis for Assessment

The quote, "Learning is something that students do, not something that is done to them", attributed to Alfie Kohn, brings out an important point. No matter how excellent the teaching and course design are, a student will likely fail to attain the course learning outcomes if they do not make an effort to do so. The primary purpose of this assessment was to determine what might be done to make it easier for students *who do make an effort* to meet the learning objectives of the course. Doing so might also lead to deeper and longer-lasting learning[refs]. The assessment was guided by three well-established principles of learning, referred to here as the mastery, feedback and motivation principles.

* **The Mastery Principle** states that to develop mastery, students must acquire component skills, practice integrating them and know when to apply what they have learned[refs].
* **The Feedback Principle** states that states that goal-directed practice coupled with targeted feedback enhances the quality of students' learning[refs].
* **The Motivation Principle** states that students' motivation determines, directs and sustains what they do to learn[refs].

Motivation has many aspects[refs]; this assessment uses achievement goal theory[refs] to characterize motivation. According to achievement goal theory, students facing a task will adopt one of four achievement goal orientations[refs]: performance-approach (demonstrate competence), mastery-approach (develop competence), performance-avoidance (don't show incompetence), or mastery-avoidance (don't fail to fully develop competence). When the task is solving a homework problem, "getting the correct answer" is a performance-approach goal, "knowing how to solve the problem" is a mastery-approach goal, "not getting an incorrect answer" is a performance-avoidance goal and "not forgetting how to solve the problem" is a mastery-avoidance goal. Achievement goal theory further posits that instructor messaging, course policy, etc. create a course goal structure that makes one of the goal orientations salient in the class or course[refs]. A student's perception of this goal structure influences the goal orientation they adopt[refs]. Generally, a mastery-approach goal orientation is desired by instructors because it is associated with favorable student learning behaviors and study strategies[refs].

### The Assessment Method

The assessment considered both the instructor's delivery of the course and the students' approaches to learning (as observed by the instructor). Each aspect of the course was examined to determine whether it was detrimental, lacking or beneficial with respect to the following critical features extracted from the principles of learning, above. 

* learning component skills - here the component skills were considered to include understanding what reactor mole and energy balances are, being able to identify mole and energy balance equations for different types of ideal reactors, knowing how to simplify the equations when appropriate, knowing defining equations for quantities that appear in the balance equations or are used in the practice of reaction engineering, knowing how to solve the equations, knowing how to minimize or maximize quantities using the equations and knowing how to estimate parameters in the equations using experimental data.
* knowing which component skills to use and when to use them - here this is taken to include selecting the correct mole and energy balances for a given problem, knowing whether simplifications are permissible, knowing when and how to express variables in terms of different variables to formulate the equations and then knowing whether to estimate parameters appearing in the equations, solve the equations, minimize or maximize a quantity appearing in the equations or specify values for some quantities appearing in the equations to meet stated objectives.
* practicing of the integration of those component skills with the mastery-approach goal of knowing what to do and how to do it in order to answer a question - here it is assumed that getting the correct answer is a natural consequence if knowing how to get the answer is the primary goal.
* providing timely, targeted feedback - here, in addition to identifying specific mistakes that were made on any one problem, the "targeting" is taken to include an assessment of current problem-solving mastery and ideas for improving it.
* creating a mastery-approach course goal structure.

### Assessment of the Course

**Didactic Lectures** The lectures organized and communicated component knowledge effectively. This was reinforced by the availability of many very good textbooks to which students were referred, typically with one being strongly recommended or required. The lectures, and many of the examples available in the textbooks, were also effective in describing and illustrating component skills wherein that knowledge is used to solve problems and answer questions. However, while they clearly showed *what to do*, they consistently failed to show or explain *how the expert knew to do those things*. Herein, this shortcoming will be referred to as failing to expose expert thinking.

Additionally, the instructor observed that students rarely, if ever, asked him how he knew to do what he did when solving a problem[refs]. The reason may well be that they don't realize this gap exists in their understanding. That is, they may fully understand all of the steps that the instructor took when solving the problem. It isn't until they are called upon to solve a problem from scratch that they realize that don't know how to start or proceed. Unfortunately, if the students use the mimicry approach described below when they solve homework problems, and if they prepare for exams by reading solved problems, the first time they may become aware of this gap in their knowledge may be on an exam.

The author also observed that many students do not prepare prior to class, e. g. by completing pre-class reading assignments. As a consequence, they may not have identified points that are not clear to them and they may miss the opportunity to ask for clarification during the lecture.

**Homework** The average homework submission rate was greater than 80% for all course offerings except in 1994 when it was 79.3%. Prior to 2005, the number of homework assignments was between 9 and 12. There were multiple questions per assignment with assignments due once per week. The instructor observed that many students completed the assignments just before they were due. During lectures, the instructor was the person solving problems while the students watched. As a consequence, many students went a week without actively engaging with the course until the next assignment came due. In an attempt to force more regular engagement, in 2005 the instructor switched to assignments that consisted of a single problem with an assignment due at each class meeting. This increased the number of assignments to 22, but did not appear to affect the submission rate which remained high (84.4%).

Homework assignments were flawed in multiple respects. While they afforded some practice, they were also graded and counted in a student's final grade for the course. As such, they were not true practice in the sense that they did not provide an opportunity to fail and learn from one's failure without penalty. Additionally, the fact that they were graded on the accuracy of the answer obtained might have contributed to establishing a performance-approach goal structure. That is, if the homework is graded on accuracy and counts toward the final grade, students are likely to make getting the correct answer their goal when solving it. The homework assignments contained little, if any, messaging on the importance of knowing how to get the correct answer. 

The instructor observed that many students employed a mimicry approach when solving homework[refs]. That is, instead of trying to solve the problem themselves, they began by locating a solved problem that appeared to be similar to the assigned problem and then mimicked that solution. Worse, students sometimes found a solution to the assigned problem and essentially copied it. While mimicry can eventually lead to learning, there are more effective learning strategies[refs]. In essence, mimicry is a very efficient way to get the correct answer, but is less effective for developing mastery. The popularity of the mimicry approach strongly suggests that the students had adopted a performance-approach goal orientation (get the correct answer) and not a mastery-approach orientation. It is critically important to note that when a student uses the mimicry approach, they don't need to know what to do, but only how to do it. That is, they are not required to practice expert thinking because the solution they are mimicking provides it.

As already noted, the instructor observed that many students did not begin homework solutions until the last minute. As a result, if they realized while solving a problem that they didn't fully understand something, they were unable to seek clarification or ask questions before the homework was due. As noted next, they may or may not have gotten that clarification from the feedback on their homework, or by seeking help outside of the classroom.

**Homework Feedback** After each homework assignment was submitted, it was graded and marked at points where mistakes had been made. Feedback on the students' solutions then consisted of returning the marked submissions and posting a correct solution that they could refer to. This form of feedback is also flawed in many respects. Like the examples presented in lecture, the posted solutions showed failed to expose expert thinking. Marking mistakes and posting a solution puts the onus on the student to compare their solution to the posted solution, determine what they did wrong and understand what they should have done instead. The learning principles state that feedback should be targeted. This form of feedback targets the reason the student failed to get the correct answer. It does not target whether or not the student knew **how** to get the correct answer. As such, its targeting may contribute to a performance-approach course goal structure and student adoption of performance-approach goals. 

Feedback also should be timely[refs]. Generally the marking of homework submissions was completed in a week or less, and always before the next exam. Still, there was a delay between when the students complete the assignment and when the feedback was available. The instructor observation that many students do not pick up their marked solutions in a timely manner exacerbates this shortcoming[refs]. By the time the feedback is received, the student may not recall their thought processes at the time they were solving the problem, making it difficult to identify a misconception of how to solve that type of problem. In some cases, the students never received any feedback either because they did not pick up their graded homework or because they did not look at it or the posted solution[refs].

**Help-Seeking Opportunities** In addition to asking whether students had any questions during each lecture, the instructor and the teaching assistants held regular office hours. This provided three or more different times each week when students could seek help. The instructor observed that during office hours some students asked questions in order to clarify their understanding of the component knowledge and skills or how to use them to solve problems. However, other students' questions revolved around the number of points they lost for a particular marked mistake[refs]. The author takes this as an indicator of those students holding a performance-approach goal orientation. In some cases, the latter questions indicated that the students had not looked at the posted solution, which, in turn, indicates that they did make use of the feedback it provided[refs].

**Exams** The questions on exams were similar to questions on homework assignments and each exam question could be mapped to a specific course learning objective. There were no "trick" questions (at least not intentionally) and the instructor attempted to maintain a consistent level of difficulty both between homework and exams and across course offerings. The lecture content was effectively the same in all course offerings. Given those factors and the similarity of the deviations in Figure \@ref(fig:deviations), the deviation of a student's exams score from the mean of the exams scores for their class offering was taken to be good indicator of that student's attainment of the course learning objectives.

``{r trendline}
sub_dev_fit <- lm(dev ~ sub, data = by_offering_data)
sub_dev_coef <- summary(sub_dev_fit)$adj.r.squared
``

Unfortunately, the available data did not provide measures of either student effort or student motivation. A student's homework submission rate might represent a crude indicator of effort, in which case some association between homework submission rate and exams scores might be expected. Figure \@ref(fig:scatter) suggests neither a strong correlation nor a highly positive one. Indeed, the least-squares trendline shown in the figure has a slope of `r round(sub_dev_fit$coefficients[2],3)` and R^2^ is equal to `r  round(sub_dev_coef,3)`.

``{r scatter, fig.cap="Association between Submission Rate and Exams Score Deviations"}
subDevCorrel <- ggplot(by_offering_data, mapping = aes(x=sub, y=dev)) +
  geom_point() +
  labs(x = "Homework Submission Rate (%)",
       y = "Exams Score Deviation (%)") +
  geom_abline(slope = sub_dev_fit$coefficients[2], intercept = sub_dev_fit$coefficients[1])
subDevCorrel
``

When examining the scatterplot it should be recognized that some students will submit a homework solution in hope of getting some points for it even though they did not make a strong effort to actually solve the problem or problems[refs]. Further, only certain discrete homework submission rates are possible, with the possible rates determined by the total number of assignments. Despite the weak overall correlation, examination of the deviations for students with a 100% homework submission rate is revealing. The distribution of exams scores deviations among student with a 100% submission rate is broader than the distribution of all other students combined. Put differently the student with the largest positive deviation and the student with the most negative deviation **both** submitted 100% of the homework! This observation, together with the relatively low trendline slope, clearly establishes that submission of homework assignments does not necessarily result in attainment of the course learning outcomes.

This raises the question, "are homework assignments ineffective teaching and learning tools, or alternatively, is there a better measure of a student's effort to learn through completion of homework assignments than their homework submission rate?" The only measure of effort in the present data is the homework submission rate, so this question remains unanswered. On the basis of the learning principled, the author believes that homework problems can be highly beneficial and effective learning tools, but in the present course both the manner in which homework is implemented and the metric used to measure homework can be improved.

The instructor observed that on exams, some students attempt to solve a problem using an approach that applies to a different type of problem (e.g. they attempt to solve a reactor response problem using an approach that applies to a parameter estimation problem). The instructor found that some students worked to the very end of the time allotted for exams and still were unable to complete their solution. Some students commented that they always get anxious during exams and it affects their performance[refs].

The instructor also noted that many students study for exams by reading and examining problems for which a solution is available[refs]. They strive to ensure that they understand those solutions, but never determine whether they can solve problems independently without having a solution to mimic. If the solved problems do not expose expert thinking, studying in this way may be incomplete. Indeed, in the days immediately before exams, the instructor did caution the classes that understanding and being able to explain someone else's solution to a problem is not the same as being able to generate that solution independently.

**Exams Feedback** Feedback on exam solutions was similar to homework feedback. The biggest difference is that in addition to posting the solution, the instructor presented an accelerated solution to the exam problems as part of a class lecture. The shortcomings and flaws associated with this feedback were essentially the same as for homework feedback.

**Summary** The following list offers a summary of finding of findings related to the structure and teaching of the course.

* The didactic lectures effectively communicate component knowledge and skills needed in the course.
* Lecture and textbook examples and posted homework and exam solutions all fail to expose expert thinking.
* The students are not provided with any opportunity for authentic practice where they can fail and learn from their mistakes without it counting as part of their course grade.
* Grading all homework for accuracy and including the resulting scores as part of the overall course grade may contribute to establishing a performance-approach course goal structure.
    * In the extreme, some students may perceive homework assignments as purely evaluative and not as opportunities to learn.
    * Students are rarely encouraged to treat problem-solving activities as opportunities to identify and address misconceptions they hold. 
* Feedback provided by marking homework and exam submissions and posting a correct solution is incomplete, inappropriately targeted and not timely.
* Some students are not able to complete the solution of an exam in the allotted time and others report exam anxiety.

The assessment also generated findings and observations related to student learning behaviors. These are based on observation making their prevalence known. In the following list the observations will be attributed to "some students," but "many students" or "a few students" might be more appropriate for items.

* Some students do not prepare prior to class meetings.
* Some students employ a mimicry approach when solving homework; it is efficient in terms of getting the correct answer, but there are more effective approaches in terms of knowing how to get the correct answer.
* Some students do not begin assignments far enough in advance of the due date to allow themselves the opportunity to seek help or clarification should they get stuck.
* The focus of some students' questions during office hours is points deducted for a mistake, not understanding the mistake.
* Some students do not make appropriate use of marked submissions and posted solutions as feedback.
    * Some never look at the posted solution or compare their solution to it.
    * Some look only to see if they have grounds to seek a higher grade.
* Some students do not reflect upon the extent of their understanding nor adjust their learning behaviors to increase it.
* When studying for exams, some students fail to appreciate the difference between understanding someone else's solution to a problem and being able to generate that solution themself.

### Course Revision Goals

The preceding assessment identified a number of issues associated with the delivery and teaching of the course and with the learning behaviors adopted by students. The delivery and teaching items are under the direct control of the instructor. While the learning behaviors are not directly under the instructor's control, the instructor can attempt to influence them. This section lists several course revision goals that address those issues. The rationale for each goal is presented, one or more course revisions that target the goal are identified, the reasoning behind the revisions is explained and potential issues are discussed.

**Induce Pre-Class Preparation** A student who has not been exposed to the topic being considered in class will derive less benefit from attending class than a prepared student. The assessment indicated that many students do not complete pre-class reading assignments. At the same time, the manner and extent to which a student prepares for each class meeting is not under the instructor's direct control. For these reasons, it may be desirable or necessary to provide inducements to the students to prepare for class.

At the end of most class meetings, the instructor summarizes that class, mentions the topic of the next class, reiterates the pre-class preparation assignment for the upcoming class and reminds students when the next homework assignment is due. This end-of-class procedure can be revised to explain to the students why the pre-class preparation assignment is important and how it will help them derive more from class time. They can be told that they may not understand everything they encounter in the pre-class preparation assignment, but that is good because they will then know to ask questions about the things they don't fully understand. They can be told that it will also be easier to follow along in class without getting lost if they have completed the pre-class preparation assignment. This need not be done at the end of every class meeting, but the message should be repeated regularly and particularly when the instructor senses a lack of pre-class preparation.

A second possible revision is to start giving short, online quizzes that are due just before the class meets. The objective of these quizzes is to induce pre-class preparation, so they **should not evaluate understanding** of the pre-class topics. Instead, it should be possible to find the answers to quiz questions within the pre-classs preparation assignment (e.g. a true/false question can consist of a statement taken directly from a pre-class reading). These quizzes should have a small number of questions, perhaps five, and not be time-consuming to complete. Multiple choice, true-false, matching and fill in the blank question types should be used. If possible they should be automatically graded by a course learning management system.

The main issue to consider when using pre-class quizzes of this type is their point value relative to the overall course score. The author believes that the overall course score should be a strong indicator of the degree to which a student has attained the course learning objectives. If the value of the quizzes is too low, they won't induce students to complete the pre-class preparation assignments. If the value is too high, they could significantly increase the overall course score. Since they do not evaluate understanding, this is not desirable. Having the total possible points awarded for these quizzes represent 5% or less of the overall course score seems reasonable.

**Communicate Necessary Information and Illustrate its Use** Students must know relevant theory and definitions, be able to recognize important equations, know how to simplify equations, etc. and they need to know how to use that information to solve problems and answer questions. The assessment indicated that the didactic lectures were an effective means of communicating this information, with the exception of exposing expert thinking which is the next item on this list.

However, using class time for this purpose may be sub-optimal. The teaching format could be revised so that this information transfer occurs outside of the classroom, via videos and/or readings. Class time could then be used to engage students in using the information and for providing instant access to and immediate feedback from the instructor. This teaching format, now known as a flipped classroom, has been advocated for this very reason.

Two potential issues can be identified. First, one of the advantages of didactic lectures is that the instructor curates the information available in the textbook, presenting and expounding on only the more important aspects. In most cases, simply replacing didactic lectures with assignments to read sections of the textbook does not provide an equivalent level of curation. While producing videos of didactic lectures has other drawbacks, it at least preserves the curation.

The second issue has already been mentioned: inducing pre-class preparation. If information transfer is moved out of the classroom, it becomes much more important that students do, in fact, prepare for class. This needs to be communicated to the students and the inducements for preparing must be effective. Even so, each in-class session should begin with a very brief review of the most important points from the pre-class assignment and an opportunity for the students to ask questions.

**Expose Expert Thinking** The mastery principle notes that once students have acquired necessary component knowledge, they must know when to use it. The assessment concluded that the didactic lectures effectively communicated the requisite knowledge and how to use it, but they failed to teach the student to know when to use that knowledge. That is, they failed to expose expert thinking. Potential revisions to address this goal involve adding content to the lectures and to posted solutions to homework and exam problems.

As information is presented over the duration of the course, each time a new type of problem is encountered for the first time, the lectures can be revised to explicitly explain how an expert would identify the new problem type and differentiate it from other problem types. This can then be followed by an enumeration of the general steps used by an expert when solving that type of problem. Importantly, the students should be made aware that they need to know these things to be successful in the course.

A second potential revision involves exposing expert thinking each time the instructor illustrates the solution of a problem. Between reading the problem and describing the first step toward solving it, a brief presentation/explanation of how the instructor knows what to do can be inserted. If in-class learning activities include problem-solving, as proposed in the next item listed, the students can be given an opportunity to practice expert thinking as part of the activity. A third potential revision involves adding expert thinking between the problem statement and the first step in the solution to every worked problem, homework problem solution and exam problem solution provided to the students. The main issue with this revision is that many textbook examples do not expose expert thinking directly. The instructor may want to provide addenda for examples in pre-class readings that do not expose expert thinking. If illustration of the use of information is being moved from an in-class didactic lecture to a pre-class video, the expert thinking can be added at the time the video is recorded.

**Provide Authentic Practice** Both the mastery principle and the feedback principle indicate that practice is essential. As used here, "authentic" practice means a student can attempt to solve a problem and make a significant, even outlandish, mistake and not be penalized for it. When coupled with a mastery-approach goal orientation and appropriate feedback, the only impact on the student who makes the mistake is that they learn from it. If the course is revised to use a flipped classroom format as suggested above, the majority of in-class time can then be devoted to problem solving activities that are neither turned in nor graded in any way. When a new problem type is first encountered, these activities can be scaffolded to guide the students through the solution. Then, as they gain experience with that problem type, the scaffolding can be removed. The scaffolding should expose expert thinking by including identification of the problem type and enumeration of the general process for obtaining an answer. If the technology is available, another option is to use clickers in the scaffolded portion of an activity. If student participation in activities like these is observed to be an issue, a possible inducement to participate might involve awarding participation points for answering clicker questions (irrespective of whether the answer was correct). In-class problem solving actvities also present opportunities for mastery coaching and timely, targeted feedback, as described below.

A second potential course revision to provide authentic practice involves not grading the accuracy of every homework assignment. This can take a number of forms, each with its own issues: (i) making some/all homework assignments optional, (ii) requiring submission of some/all assignments and awarding points for submitting it without grading it for accuracy or (iii) requiring submission of some/all assignments but grading them **only** on the basis of the apparent effort put forth and not their accuracy. The issues with all of these revisions come down to inducements. With no grade and no inducement, some students won't complete the assignments. Large inducements introduce the possibility of increasing a student's overall course score despite their solutions being incorrect. As with inducements considered previously, a careful and thoughtful balance is required. By eliminating homework assignments the previously counted toward overall course score, these revisions may cause exam scores to become more heavily weighted in the overall evaluation of the students. Care must be taken to ensure the exams represent a true and fair measure of a student's attainment of the learning objectives.

For any homework assignment that is optional, required but not graded or graded on effort only, the problem statement should explicitly say that the purpose of that assignment is for the students to practice solving the problem so they can gauge their readiness for answering a similar question on an exam. They should be told that while getting the correct answer is important, knowing how to solve the problem is more important. If other assignments are graded for correctness and accuracy, the students should periodically be reminded that homework assignments are not **only** intended to evaluate their abilities, but also to provide them with opportunities to practice and develop those abilities.

**Provide Feedback that is Timely, Complete and Properly Targeted** Feedback can help students identify their misunderstandings and misconceptions, as well as motivate (or de-motivate) them to persevere. The assessment indicated that in this course, the feedback was not timely, it helped the students find mistakes, but left it to them to associate those mistakes with an underlying misunderstanding. It implicitly targeted getting the correct answer. Several potential revisions could address these shortcomings.

If the course is flipped and class time is largely devoted to problem-solving activities, the instructor can circulate among the students checking their progress and answering their questions. If a question arises several times or the instructor observes multiple students sharing the same misconception, the activity can be interupted with a brief mini-lecture addressing that misconception. (The same can be done with clicker questions if a large percentage of the participants do not answer correctly.) In this way, the students can receive instantaneous feedback that is targeted to correct misunderstanding and increase mastery. The instructor can also comment on how their mastery is improving to individuals or the whole class. Alternatively the instructor can tell the class to reflect upon their progress since they first begain solving a particular type of problem, etc. These revision may provide motivation.

Another potential revision is to implement the use of homework wrappers with each homework assignment. As used here, a homework wrapper is a short writing within which the student reflects upon their solution to the problem. The question should be written so that in order to respond appropriately, the students will need to compare their solution to the posted, correct solution, assess their ability to solve problems of this type and determine what they can do to improve their mastery if need be. As such, homework wrappers can help students make better use of the feedback a posted solution provides and simultaneously lead them to self-generate feedback that is targeted on mastery over performance. The instructor can also respond to submitted wrappers with encouraging comments, comments intended to clarify a point, suggestion to meet during office hours, etc.

**Equitably Evaluate Learning Outcomes** In the course assessment the instructor noted that some students could not complete exams in the allotted time and other students stated that anxiety often results in exam solutions that don't accuratey reflect their abilities and understanding. It was also noted above that if less (or none) of the homework assignments are graded and included in overall course score, then exams will assume a larger fraction of the students' final grades.

Potentially the type and format of student evaluation can be revised to better reflect a student's attainment of the course learning objectives. One of the few positive results of the COVID-19 pandemic was that some instructors were uncomfortable with giving exams via teleconference and devised alternative forms of evaluation including projects, papers, etc. Those changes are all potential revisions that can establish a more equitable evaluation of student learning. One change that the author made was to increase the number of exams, with only one problem on each exam. Further, the exams were split into an in-class portion and a take home portion. On the in-class portion the students were told to fully formulate the solution to the problem, but not to perform any of the associated calculations. They were then given 24 hours for the take-home part of the exam wherein they were to perform the calculations. This potential revision, by having only one problem and not requiring in-class calculations, greatly reduces the pressure or need to rush that some students feel while taking the exam. It reduces the reward for speed in favor of rewarding understanding. 

**Establish a Mastery-Approach Course Goal Structure** According to achievement goal theory, students perceive a course goal structure based on instructor messaging, course policy, etc. The perceived goal structure influences the goal orientation that students adopt, and that, in turn, affects their learning behaviors and study methods. A master-approach goal orientation is associated with desirable study strategies, so the instructor should strive to establish a master-approach class goal structure. There are many possible course revisions to accomplish this.

One set of potential course revisions can be grouped under the heading of "mastery coaching." Mastery coaching involves repeated statements by the instructor that emphasize learning and understanding how to solve problems. In a flipped classroom, the instructor can engage in mastery coaching while circulation among the students announcing things like "it is good to get stuck because that shows you a point you haven't yet fully mastered," and "the purpose of this activity isn't go get the answer, it's to make sure you know how to get the answer. If you know that, the correct answer will follow naturally." Similarly, in class meetings immediately before exams, the instructor can note that "understanding someone else's solution is not the same as generating the solution independently," and "a good way to study for an exam is to look at a problem for which a solution is available and try to set up the calculations for solving it without looking at the solution; only look at the solution after you are fininshed or if you get stuck."

Introducing effort-based grading as suggested above also contributes to establishing a mastery-approach goal structure. A further potential revision is to provide the solution to these problems at the time they are assigned, and telling the students that the purpose of the assignment is for them to find out whether they know how to solve this type of problem. Giving them the answer before they start the assignment clearly indicates that getting the correct answer isn't the purpose.

### Conclusions

Assessment of ten offerings of an engineering problem-solving course revealed a number of aspects of the course structure and teaching that did not fully align with established principles of how students learn. Instructor observations from those offerings additionally identified maladaptive or sub-optimal student learning behaviors. The assessment results suggest that a performance-approach goal structure exists in the class, and students adopt a performance-approach goal orientation in which getting the correct answer predominates over the mastery-approach goal of **knowing how** to get the answer. Seven revisions to the course would address many of its shortcomings. 

### Cited References

## License {.ch}

{{< include includes/_bottom_license.qmd >}}
